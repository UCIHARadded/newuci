{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# First clone the repository\n",
        "!git clone https://github.com/UCIHARadded/newuci.git\n",
        "\n",
        "%cd newuci/diversify"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZd1EgW9z_KL",
        "outputId": "5543d876-c5a9-4d25-b21c-5043d6524bcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'newuci'...\n",
            "remote: Enumerating objects: 1398, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 1398 (delta 11), reused 8 (delta 8), pack-reused 1379 (from 3)\u001b[K\n",
            "Receiving objects: 100% (1398/1398), 18.65 MiB | 11.15 MiB/s, done.\n",
            "Resolving deltas: 100% (767/767), done.\n",
            "/content/newuci/diversify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "i72KjcaU0JJR",
        "outputId": "84723216-f647-41d5-f9af-38896ef2a4fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (0.21.0+cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.15.3)\n",
            "Collecting numpy==1.23.5 (from -r requirements.txt (line 4))\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.1->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->-r requirements.txt (line 2)) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.1->-r requirements.txt (line 1)) (3.0.2)\n",
            "Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.7 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.3.3 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.4.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "bcd31c1d0fe24f288d001b7a4135a457"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean any existing files or folders\n",
        "!rm -rf data/UCI\\ HAR\\ Dataset data/UCI-HAR-dataset UCI-HAR.zip\n",
        "\n",
        "# Recreate the data directory\n",
        "!mkdir -p data/\n",
        "\n",
        "# Download the UCI HAR dataset\n",
        "!wget -O UCI-HAR.zip https://archive.ics.uci.edu/static/public/240/human+activity+recognition+using+smartphones.zip\n",
        "\n",
        "# Check contents of the ZIP (to confirm folder name inside ZIP)\n",
        "!unzip -l UCI-HAR.zip | head -n 20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_h7w2tdlzaH",
        "outputId": "4c053017-2b8c-40a1-c624-d08f6b5ec3b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-02 11:38:51--  https://archive.ics.uci.edu/static/public/240/human+activity+recognition+using+smartphones.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘UCI-HAR.zip’\n",
            "\n",
            "UCI-HAR.zip             [                 <=>]  58.18M  17.9MB/s    in 4.0s    \n",
            "\n",
            "2025-07-02 11:38:56 (14.4 MB/s) - ‘UCI-HAR.zip’ saved [61005872]\n",
            "\n",
            "Archive:  UCI-HAR.zip\n",
            "  Length      Date    Time    Name\n",
            "---------  ---------- -----   ----\n",
            "     6304  2023-05-22 15:22   UCI HAR Dataset.names\n",
            " 60999314  2023-05-22 15:22   UCI HAR Dataset.zip\n",
            "---------                     -------\n",
            " 61005618                     2 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Extract outer ZIP\n",
        "!unzip -o UCI-HAR.zip -d data/\n",
        "\n",
        "# Step 2: Extract the inner ZIP\n",
        "!unzip -o \"data/UCI HAR Dataset.zip\" -d data/\n",
        "\n",
        "# Step 3: Rename folder to standard name\n",
        "!mv \"data/UCI HAR Dataset\" data/UCI-HAR-dataset\n",
        "\n",
        "# Step 4: Verify structure\n",
        "!ls data/UCI-HAR-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BSGytFvmBkz",
        "outputId": "9516789d-09f5-49ef-e8a1-838801bd38eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  UCI-HAR.zip\n",
            " extracting: data/UCI HAR Dataset.names  \n",
            " extracting: data/UCI HAR Dataset.zip  \n",
            "Archive:  data/UCI HAR Dataset.zip\n",
            "   creating: data/UCI HAR Dataset/\n",
            "  inflating: data/UCI HAR Dataset/.DS_Store  \n",
            "   creating: data/__MACOSX/\n",
            "   creating: data/__MACOSX/UCI HAR Dataset/\n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/._.DS_Store  \n",
            "  inflating: data/UCI HAR Dataset/activity_labels.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/._activity_labels.txt  \n",
            "  inflating: data/UCI HAR Dataset/features.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/._features.txt  \n",
            "  inflating: data/UCI HAR Dataset/features_info.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/._features_info.txt  \n",
            "  inflating: data/UCI HAR Dataset/README.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/._README.txt  \n",
            "   creating: data/UCI HAR Dataset/test/\n",
            "   creating: data/UCI HAR Dataset/test/Inertial Signals/\n",
            "  inflating: data/UCI HAR Dataset/test/Inertial Signals/body_acc_x_test.txt  \n",
            "   creating: data/__MACOSX/UCI HAR Dataset/test/\n",
            "   creating: data/__MACOSX/UCI HAR Dataset/test/Inertial Signals/\n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_acc_x_test.txt  \n",
            "  inflating: data/UCI HAR Dataset/test/Inertial Signals/body_acc_y_test.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_acc_y_test.txt  \n",
            "  inflating: data/UCI HAR Dataset/test/Inertial Signals/body_acc_z_test.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_acc_z_test.txt  \n",
            "  inflating: data/UCI HAR Dataset/test/Inertial Signals/body_gyro_x_test.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_gyro_x_test.txt  \n",
            "  inflating: data/UCI HAR Dataset/test/Inertial Signals/body_gyro_y_test.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_gyro_y_test.txt  \n",
            "  inflating: data/UCI HAR Dataset/test/Inertial Signals/body_gyro_z_test.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_gyro_z_test.txt  \n",
            "  inflating: data/UCI HAR Dataset/test/Inertial Signals/total_acc_x_test.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/test/Inertial Signals/._total_acc_x_test.txt  \n",
            "  inflating: data/UCI HAR Dataset/test/Inertial Signals/total_acc_y_test.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/test/Inertial Signals/._total_acc_y_test.txt  \n",
            "  inflating: data/UCI HAR Dataset/test/Inertial Signals/total_acc_z_test.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/test/Inertial Signals/._total_acc_z_test.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/test/._Inertial Signals  \n",
            "  inflating: data/UCI HAR Dataset/test/subject_test.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/test/._subject_test.txt  \n",
            "  inflating: data/UCI HAR Dataset/test/X_test.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/test/._X_test.txt  \n",
            "  inflating: data/UCI HAR Dataset/test/y_test.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/test/._y_test.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/._test  \n",
            "   creating: data/UCI HAR Dataset/train/\n",
            "   creating: data/UCI HAR Dataset/train/Inertial Signals/\n",
            "  inflating: data/UCI HAR Dataset/train/Inertial Signals/body_acc_x_train.txt  \n",
            "   creating: data/__MACOSX/UCI HAR Dataset/train/\n",
            "   creating: data/__MACOSX/UCI HAR Dataset/train/Inertial Signals/\n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_acc_x_train.txt  \n",
            "  inflating: data/UCI HAR Dataset/train/Inertial Signals/body_acc_y_train.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_acc_y_train.txt  \n",
            "  inflating: data/UCI HAR Dataset/train/Inertial Signals/body_acc_z_train.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_acc_z_train.txt  \n",
            "  inflating: data/UCI HAR Dataset/train/Inertial Signals/body_gyro_x_train.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_gyro_x_train.txt  \n",
            "  inflating: data/UCI HAR Dataset/train/Inertial Signals/body_gyro_y_train.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_gyro_y_train.txt  \n",
            "  inflating: data/UCI HAR Dataset/train/Inertial Signals/body_gyro_z_train.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_gyro_z_train.txt  \n",
            "  inflating: data/UCI HAR Dataset/train/Inertial Signals/total_acc_x_train.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/train/Inertial Signals/._total_acc_x_train.txt  \n",
            "  inflating: data/UCI HAR Dataset/train/Inertial Signals/total_acc_y_train.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/train/Inertial Signals/._total_acc_y_train.txt  \n",
            "  inflating: data/UCI HAR Dataset/train/Inertial Signals/total_acc_z_train.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/train/Inertial Signals/._total_acc_z_train.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/train/._Inertial Signals  \n",
            "  inflating: data/UCI HAR Dataset/train/subject_train.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/train/._subject_train.txt  \n",
            "  inflating: data/UCI HAR Dataset/train/X_train.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/train/._X_train.txt  \n",
            "  inflating: data/UCI HAR Dataset/train/y_train.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/train/._y_train.txt  \n",
            "  inflating: data/__MACOSX/UCI HAR Dataset/._train  \n",
            "  inflating: data/__MACOSX/._UCI HAR Dataset  \n",
            "activity_labels.txt  features_info.txt\tfeatures.txt  README.txt  test\ttrain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "  --data_dir ./data/UCI-HAR-dataset/ \\\n",
        "  --task cross_people \\\n",
        "  --test_envs 0 \\\n",
        "  --dataset uci_har \\\n",
        "  --algorithm diversify \\\n",
        "  --latent_domain_num 10 \\\n",
        "  --alpha1 1.0 \\\n",
        "  --alpha 1.0 \\\n",
        "  --lam 0.0 \\\n",
        "  --local_epoch 3 \\\n",
        "  --max_epoch 2 \\\n",
        "  --lr 0.01 \\\n",
        "  --output ./data/train_output/act/cross_people-uci_har-Diversify-0-10-1-1-0-3-2-0.01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oyyzSPxxGWr",
        "outputId": "eb2fa7f1-f5d8-4a5f-c063-f3c45e2b7c21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment:\n",
            "\tPython: 3.11.13\n",
            "\tPyTorch: 2.6.0+cu124\n",
            "\tTorchvision: 0.21.0+cu124\n",
            "\tCUDA: 12.4\n",
            "\tCUDNN: 90300\n",
            "\tNumPy: 2.0.2\n",
            "\tPIL: 11.2.1\n",
            "==========================================\n",
            "algorithm:diversify\n",
            "alpha:1.0\n",
            "alpha1:1.0\n",
            "batch_size:32\n",
            "beta1:0.5\n",
            "bottleneck:256\n",
            "checkpoint_freq:100\n",
            "classifier:linear\n",
            "data_file:\n",
            "dataset:uci_har\n",
            "data_dir:./data/UCI-HAR-dataset/\n",
            "dis_hidden:256\n",
            "gpu_id:0\n",
            "layer:bn\n",
            "lam:0.0\n",
            "latent_domain_num:10\n",
            "local_epoch:3\n",
            "lr:0.01\n",
            "lr_decay1:1.0\n",
            "lr_decay2:1.0\n",
            "max_epoch:2\n",
            "model_size:median\n",
            "N_WORKERS:4\n",
            "old:False\n",
            "seed:0\n",
            "task:cross_people\n",
            "test_envs:[0]\n",
            "output:./data/train_output/act/cross_people-uci_har-Diversify-0-10-1-1-0-3-2-0.01\n",
            "weight_decay:0.0005\n",
            "domain_num:30\n",
            "steps_per_epoch:10000000000\n",
            "select_position:{'uci_har': [0]}\n",
            "select_channel:{'uci_har': array([0])}\n",
            "hz_list:{'uci_har': 50}\n",
            "act_people:{'uci_har': [[0]]}\n",
            "num_classes:6\n",
            "input_shape:(570, 1, 128)\n",
            "grid_size:1\n",
            "\n",
            "[INFO] Using UCI HAR dataset loader\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[CONFIG] Using 30 domains with 6 classes\n",
            "[INIT] dclassifier initialized with input dim: 256\n",
            "\n",
            "======== ROUND 0 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.7654349804    \n",
            "1                1.8865001202    \n",
            "2                1.6119954586    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                4.0048923492     1.9283708334     2.0765213966    \n",
            "1                3.9231634140     1.8860477209     2.0371158123    \n",
            "2                3.0961356163     1.4587210417     1.6374146938    \n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "0                0.1753364503     3.4027404785     0.1753364503     0.9496735582     0.9036308110     0.9036308110     17.9786541462   \n",
            "1                0.1432683170     3.4011025429     0.1432683170     0.9473612622     0.9056667798     0.9056667798     36.9340846539   \n",
            "2                0.1790707111     3.4009685516     0.1790707111     0.9322633297     0.8717339667     0.8717339667     54.8553130627   \n",
            "\n",
            "======== ROUND 1 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.4223572016    \n",
            "1                1.0117326975    \n",
            "2                1.1188198328    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.5358209610     1.6557607651     1.8800600767    \n",
            "1                3.4601731300     1.7340883017     1.7260847092    \n",
            "2                3.5370452404     1.6757625341     1.8612827063    \n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "0                0.1211411953     3.4009609222     0.1211411953     0.9446409140     0.8958262640     0.8958262640     17.8966066837   \n",
            "1                0.1594937146     3.4010741711     0.1594937146     0.9432807399     0.8971835765     0.8971835765     36.6601655483   \n",
            "2                0.1298651546     3.4013535976     0.1298651546     0.9424646355     0.9093993892     0.9093993892     54.8455135822   \n",
            "\n",
            "🎯 Final Target Accuracy: 0.9094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --data_dir ./data/UCI-HAR-dataset/ --task cross_people --test_envs 1 --dataset uci_har --algorithm diversify --latent_domain_num 2 --alpha1 0.1 --alpha 10.0 --lam 0.0 --local_epoch 10 --max_epoch 1 --lr 0.01 --output ./data/train_output/act/cross_people-emg-Diversify-1-2-0.1-10-0-10-15-0.01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KKOyoeD8Aac",
        "outputId": "834d3e74-4f2e-4a35-b8b9-9fb569d3b51b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment:\n",
            "\tPython: 3.11.13\n",
            "\tPyTorch: 2.6.0+cu124\n",
            "\tTorchvision: 0.21.0+cu124\n",
            "\tCUDA: 12.4\n",
            "\tCUDNN: 90300\n",
            "\tNumPy: 2.0.2\n",
            "\tPIL: 11.2.1\n",
            "==========================================\n",
            "algorithm:diversify\n",
            "alpha:10.0\n",
            "alpha1:0.1\n",
            "batch_size:32\n",
            "beta1:0.5\n",
            "bottleneck:256\n",
            "checkpoint_freq:100\n",
            "classifier:linear\n",
            "data_file:\n",
            "dataset:uci_har\n",
            "data_dir:./data/UCI-HAR-dataset/\n",
            "dis_hidden:256\n",
            "gpu_id:0\n",
            "layer:bn\n",
            "lam:0.0\n",
            "latent_domain_num:2\n",
            "local_epoch:10\n",
            "lr:0.01\n",
            "lr_decay1:1.0\n",
            "lr_decay2:1.0\n",
            "max_epoch:1\n",
            "model_size:median\n",
            "N_WORKERS:4\n",
            "old:False\n",
            "seed:0\n",
            "task:cross_people\n",
            "test_envs:[1]\n",
            "output:./data/train_output/act/cross_people-emg-Diversify-1-2-0.1-10-0-10-15-0.01\n",
            "weight_decay:0.0005\n",
            "domain_num:30\n",
            "steps_per_epoch:10000000000\n",
            "select_position:{'uci_har': [0]}\n",
            "select_channel:{'uci_har': array([0])}\n",
            "hz_list:{'uci_har': 50}\n",
            "act_people:{'uci_har': [[0]]}\n",
            "num_classes:6\n",
            "input_shape:(570, 1, 128)\n",
            "grid_size:1\n",
            "\n",
            "[INFO] Using UCI HAR dataset loader\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[CONFIG] Using 30 domains with 6 classes\n",
            "[INIT] dclassifier initialized with input dim: 256\n",
            "\n",
            "======== ROUND 0 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                3.0419311523    \n",
            "1                1.8929001093    \n",
            "2                1.7280887365    \n",
            "3                1.6061831713    \n",
            "4                1.3657485247    \n",
            "5                1.1552642584    \n",
            "6                1.1156746149    \n",
            "7                1.5387132168    \n",
            "8                1.1692726612    \n",
            "9                1.2269767523    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.4894018173     1.7477425337     1.7416592836    \n",
            "1                3.2151303291     1.4133123159     1.8018178940    \n",
            "2                2.8480706215     1.2976009846     1.5504695177    \n",
            "3                2.7990846634     1.1187151670     1.6803693771    \n",
            "4                2.9484934807     1.4056179523     1.5428755283    \n",
            "5                2.3963727951     1.0032107830     1.3931620121    \n",
            "6                2.3349728584     0.9322222471     1.4027507305    \n",
            "7                2.3031761646     1.0077449083     1.2954312563    \n",
            "8                2.1217961311     0.7730774879     1.3487186432    \n",
            "9                2.2153840065     0.8288990259     1.3864848614    \n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "0                0.3113740087     3.4012665749     0.3113740087     0.9479053319     0.8887003733     0.8887003733     19.8257124424   \n",
            "1                0.4653229415     3.4012675285     0.4653229415     0.9360718172     0.8561248728     0.8561248728     39.9197125435   \n",
            "2                0.1194547415     3.4010012150     0.1194547415     0.9427366703     0.8883610451     0.8883610451     60.9974730015   \n",
            "3                0.1773362607     3.4009172916     0.1773362607     0.9586507073     0.9087207329     0.9087207329     80.9694070816   \n",
            "4                0.1486248970     3.4009518623     0.1486248970     0.9508977149     0.8887003733     0.8887003733     101.1058866978  \n",
            "5                0.0708849356     3.4014627934     0.0708849356     0.9540261153     0.9053274516     0.9053274516     121.8109383583  \n",
            "6                0.1251315027     3.4010357857     0.1251315027     0.9515778020     0.8754665762     0.8754665762     141.6052606106  \n",
            "7                0.2507629097     3.4015846252     0.2507629097     0.9583786725     0.8866644045     0.8866644045     162.0084831715  \n",
            "8                0.2321300954     3.4015290737     0.2321300954     0.9566104461     0.8903970139     0.8903970139     182.6924827099  \n",
            "9                0.0962740257     3.4008905888     0.0962740257     0.9547062024     0.9049881235     0.9049881235     202.4454820156  \n",
            "\n",
            "🎯 Final Target Accuracy: 0.9087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --data_dir ./data/UCI-HAR-dataset/ --task cross_people --test_envs 2 --dataset uci_har --algorithm diversify --latent_domain_num 20 --alpha1 0.5 --alpha 1.0 --lam 0.0 --local_epoch 1 --max_epoch 1 --lr 0.01 --output ./data/train_output/act/cross_people-emg-Diversify-2-20-0.5-1-0-1-150-0.01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzoRqNjG8AVq",
        "outputId": "c29b2d47-88f4-4018-932a-a29c7c7c4fa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment:\n",
            "\tPython: 3.11.13\n",
            "\tPyTorch: 2.6.0+cu124\n",
            "\tTorchvision: 0.21.0+cu124\n",
            "\tCUDA: 12.4\n",
            "\tCUDNN: 90300\n",
            "\tNumPy: 2.0.2\n",
            "\tPIL: 11.2.1\n",
            "==========================================\n",
            "algorithm:diversify\n",
            "alpha:1.0\n",
            "alpha1:0.5\n",
            "batch_size:32\n",
            "beta1:0.5\n",
            "bottleneck:256\n",
            "checkpoint_freq:100\n",
            "classifier:linear\n",
            "data_file:\n",
            "dataset:uci_har\n",
            "data_dir:./data/UCI-HAR-dataset/\n",
            "dis_hidden:256\n",
            "gpu_id:0\n",
            "layer:bn\n",
            "lam:0.0\n",
            "latent_domain_num:20\n",
            "local_epoch:1\n",
            "lr:0.01\n",
            "lr_decay1:1.0\n",
            "lr_decay2:1.0\n",
            "max_epoch:1\n",
            "model_size:median\n",
            "N_WORKERS:4\n",
            "old:False\n",
            "seed:0\n",
            "task:cross_people\n",
            "test_envs:[2]\n",
            "output:./data/train_output/act/cross_people-emg-Diversify-2-20-0.5-1-0-1-150-0.01\n",
            "weight_decay:0.0005\n",
            "domain_num:30\n",
            "steps_per_epoch:10000000000\n",
            "select_position:{'uci_har': [0]}\n",
            "select_channel:{'uci_har': array([0])}\n",
            "hz_list:{'uci_har': 50}\n",
            "act_people:{'uci_har': [[0]]}\n",
            "num_classes:6\n",
            "input_shape:(570, 1, 128)\n",
            "grid_size:1\n",
            "\n",
            "[INFO] Using UCI HAR dataset loader\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[CONFIG] Using 30 domains with 6 classes\n",
            "[INIT] dclassifier initialized with input dim: 256\n",
            "\n",
            "======== ROUND 0 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.8109898567    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                4.6246852875     2.2889657021     2.3357195854    \n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "0                0.2901648581     3.3995647430     0.2901648581     0.8934983678     0.8154054971     0.8154054971     7.9229791164    \n",
            "\n",
            "🎯 Final Target Accuracy: 0.8154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --data_dir ./data/UCI-HAR-dataset/ --task cross_people --test_envs 3 --dataset uci_har --algorithm diversify --latent_domain_num 5 --alpha1 5.0 --alpha 0.1 --lam 0.0 --local_epoch 5 --max_epoch 2 --lr 0.01 --output ./data/train_output/act/cross_people-emg-Diversify-3-5-5-0.1-0-5-30-0.01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eusODM2I8ARD",
        "outputId": "c2932ea3-7dce-42a3-8b0e-9b16207fe562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment:\n",
            "\tPython: 3.11.13\n",
            "\tPyTorch: 2.6.0+cu124\n",
            "\tTorchvision: 0.21.0+cu124\n",
            "\tCUDA: 12.4\n",
            "\tCUDNN: 90300\n",
            "\tNumPy: 2.0.2\n",
            "\tPIL: 11.2.1\n",
            "==========================================\n",
            "algorithm:diversify\n",
            "alpha:0.1\n",
            "alpha1:5.0\n",
            "batch_size:32\n",
            "beta1:0.5\n",
            "bottleneck:256\n",
            "checkpoint_freq:100\n",
            "classifier:linear\n",
            "data_file:\n",
            "dataset:uci_har\n",
            "data_dir:./data/UCI-HAR-dataset/\n",
            "dis_hidden:256\n",
            "gpu_id:0\n",
            "layer:bn\n",
            "lam:0.0\n",
            "latent_domain_num:5\n",
            "local_epoch:5\n",
            "lr:0.01\n",
            "lr_decay1:1.0\n",
            "lr_decay2:1.0\n",
            "max_epoch:2\n",
            "model_size:median\n",
            "N_WORKERS:4\n",
            "old:False\n",
            "seed:0\n",
            "task:cross_people\n",
            "test_envs:[3]\n",
            "output:./data/train_output/act/cross_people-emg-Diversify-3-5-5-0.1-0-5-30-0.01\n",
            "weight_decay:0.0005\n",
            "domain_num:30\n",
            "steps_per_epoch:10000000000\n",
            "select_position:{'uci_har': [0]}\n",
            "select_channel:{'uci_har': array([0])}\n",
            "hz_list:{'uci_har': 50}\n",
            "act_people:{'uci_har': [[0]]}\n",
            "num_classes:6\n",
            "input_shape:(570, 1, 128)\n",
            "grid_size:1\n",
            "\n",
            "[INFO] Using UCI HAR dataset loader\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[CONFIG] Using 30 domains with 6 classes\n",
            "[INIT] dclassifier initialized with input dim: 256\n",
            "\n",
            "======== ROUND 0 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.7654349804    \n",
            "1                1.8865001202    \n",
            "2                1.6119954586    \n",
            "3                1.4704403877    \n",
            "4                1.1708402634    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                4.2515110970     1.8228813410     2.4286298752    \n",
            "1                3.9838957787     1.5981882811     2.3857073784    \n",
            "2                3.8726429939     1.6983531713     2.1742899418    \n",
            "3                2.9400331974     1.1208181381     1.8192150593    \n",
            "4                3.5586795807     1.4999369383     2.0587425232    \n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "0                0.3367739916     3.4022614956     0.3367739916     0.9351196953     0.9087207329     0.9087207329     18.6852231026   \n",
            "1                0.1793745756     3.4009041786     0.1793745756     0.9533460283     0.9039701391     0.9039701391     36.5191006660   \n",
            "2                0.1295214891     3.4009239674     0.1295214891     0.9542981502     0.9046487954     0.9046487954     55.1746494770   \n",
            "3                0.1520110518     3.4009716511     0.1520110518     0.9552502720     0.9039701391     0.9039701391     73.2212972641   \n",
            "4                0.2277206182     3.4009599686     0.2277206182     0.9564744287     0.8982015609     0.8982015609     91.9252207279   \n",
            "\n",
            "======== ROUND 1 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.0781259537    \n",
            "1                1.1725206375    \n",
            "2                1.2280921936    \n",
            "3                0.8440845013    \n",
            "4                0.9930109978    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.2620584965     1.3788900375     1.8831684589    \n",
            "1                3.3993356228     1.3427362442     2.0565993786    \n",
            "2                3.1893191338     1.2989946604     1.8903245926    \n",
            "3                3.6285386086     1.5607767105     2.0677618980    \n",
            "4                3.5955338478     1.5569232702     2.0386106968    \n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "0                0.1289389133     3.4012715816     0.1289389133     0.9532100109     0.9090600611     0.9090600611     18.7243123055   \n",
            "1                0.0919869468     3.4011013508     0.0919869468     0.9571545158     0.8873430607     0.8873430607     36.5769555569   \n",
            "2                0.2891151607     3.4009275436     0.2891151607     0.9496735582     0.9032914829     0.9032914829     55.3410761356   \n",
            "3                0.1478193849     3.4010443687     0.1478193849     0.9617791077     0.9093993892     0.9093993892     73.2197046280   \n",
            "4                0.1357624382     3.4006481171     0.1357624382     0.9668117519     0.9239904988     0.9239904988     92.5485885143   \n",
            "\n",
            "🎯 Final Target Accuracy: 0.9240\n"
          ]
        }
      ]
    }
  ]
}